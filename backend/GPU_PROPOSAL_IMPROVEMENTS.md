# GPU Rendering: Fine-Tuning Optimizations v2.0

## Executive Summary

Este documento proporciona **optimizaciones especÃ­ficas e implementables** para mejorar el rendimiento del renderizado GPU **sin romper** la lÃ³gica CPU ni el frontend existente. Todas las propuestas son **ajustes finos** basados en el anÃ¡lisis del cÃ³digo actual (`render_gpu.py`).

> **Estado Actual**: Pipeline GPU funcional con precarga, muestreo y composiciÃ³n  
> **Objetivo**: Aumentar FPS y eliminar cuellos de botella identificados  
> **Estrategia**: Micro-optimizaciones que se acumulan para mayor velocidad

---

## Table of Contents

1. [AnÃ¡lisis del CÃ³digo Actual](#1-anÃ¡lisis-del-cÃ³digo-actual)
2. [Cuellos de Botella Identificados](#2-cuellos-de-botella-identificados)
3. [Optimizaciones de Prioridad Alta (P0)](#3-optimizaciones-de-prioridad-alta-p0)
4. [Optimizaciones de Prioridad Media (P1)](#4-optimizaciones-de-prioridad-media-p1)
5. [Optimizaciones de Prioridad Baja (P2)](#5-optimizaciones-de-prioridad-baja-p2)
6. [Orden de ImplementaciÃ³n Recomendado](#6-orden-de-implementaciÃ³n-recomendado)
7. [Notas de Seguridad](#7-notas-de-seguridad)

---

## 1. AnÃ¡lisis del CÃ³digo Actual

### 1.1 Arquitectura del Pipeline GPU Actual

El sistema actual en `render_gpu.py` sigue este flujo:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE GPU ACTUAL (render_gpu.py)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                             â”‚
â”‚  â”‚   PRELOAD   â”‚ â—„â”€â”€ GPURenderContext.preload()                              â”‚
â”‚  â”‚             â”‚                                                             â”‚
â”‚  â”‚  1. Leer Ortofoto (rasterio)                                              â”‚
â”‚  â”‚  2. Convertir a RGBA (_to_rgba, _normalize_rgba)                          â”‚
â”‚  â”‚  3. Subir a GPU: cp.asarray().transpose(2,0,1) â—„â”€ [CUELLO DE BOTELLA 1]  â”‚
â”‚  â”‚  4. Generar Mipmaps (3 niveles)                                           â”‚
â”‚  â”‚  5. Procesar Vectores (PIL/CPU â†’ GPU)           â—„â”€ [CUELLO DE BOTELLA 2] â”‚
â”‚  â”‚  6. Descargar WMS (_fetch_wms_mosaic_for_bounds)                          â”‚
â”‚  â”‚  7. Subir WMS a GPU                                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                             â”‚
â”‚         â”‚                                                                    â”‚
â”‚         â–¼                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                             â”‚
â”‚  â”‚ RENDER FRAMEâ”‚ â—„â”€â”€ render_frame_gpu() - POR CADA FRAME                     â”‚
â”‚  â”‚             â”‚                                                             â”‚
â”‚  â”‚  1. Calcular nivel mipmap (scale_ratio)                                   â”‚
â”‚  â”‚  2. Muestrear WMS (_sample_wms_layer_gpu_approx)  â—„â”€ [CUELLO BOTELLA 3]  â”‚
â”‚  â”‚     â””â”€ 4 affine_transform (uno por canal RGBA)                            â”‚
â”‚  â”‚  3. Muestrear Ortofoto (_sample_using_inverse_transform)                  â”‚
â”‚  â”‚     â””â”€ 4 affine_transform                        â—„â”€ [CUELLO BOTELLA 4]   â”‚
â”‚  â”‚  4. Alpha Composite (ortho sobre WMS)                                     â”‚
â”‚  â”‚  5. Muestrear Vectores (_sample_using_inverse_transform)                  â”‚
â”‚  â”‚     â””â”€ 4 affine_transform                        â—„â”€ [CUELLO BOTELLA 5]   â”‚
â”‚  â”‚  6. Alpha Composite (vectors sobre resultado)                             â”‚
â”‚  â”‚  7. Downsample 2Ã— (_gpu_downsample_box)                                   â”‚
â”‚  â”‚  8. UI Overlay (PIL/CPU)                         â—„â”€ [CUELLO BOTELLA 6]   â”‚
â”‚  â”‚  9. Subir UI a GPU, composite final                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                             â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Puntos Fuertes del CÃ³digo Actual

| Aspecto | Estado | DescripciÃ³n |
|---------|--------|-------------|
| **Precarga a GPU** | âœ… Implementado | Ortofoto, WMS y vectores se cargan una sola vez |
| **Mipmaps** | âœ… Implementado | 3 niveles (1Ã—, 0.5Ã—, 0.25Ã—) para diferentes zoom |
| **Alpha Compositing GPU** | âœ… Implementado | `_alpha_composite_gpu()` funciona correctamente |
| **Downsample GPU** | âœ… Implementado | `_gpu_downsample_box()` eficiente para 2Ã— |
| **Pipeline FFmpeg Directo** | âœ… Implementado | En `app.py` con memoria pinned |
| **SelecciÃ³n de Mipmap** | âœ… Implementado | Basado en `scale_ratio` |

### 1.3 Funciones Clave Analizadas

```python
# render_gpu.py - LÃ­neas clave

# LÃ­nea 76-102: _alpha_composite_gpu() 
#   - Correcta implementaciÃ³n de Porter-Duff
#   - Convierte a float32 para precisiÃ³n

# LÃ­nea 104-120: _gpu_downsample_box()
#   - Eficiente para scale=2 (box filter manual)
#   - Fallback a zoom() para otros scales

# LÃ­nea 122-141: _get_transformation_basis()
#   - CÃ¡lculo de vectores base para rotaciÃ³n
#   - Matches con render.py (heading apunta UP)

# LÃ­nea 143-200: _sample_using_inverse_transform()
#   - Loop sobre 4 canales RGBA (lÃ­neas 187-198) â—„â”€ OPTIMIZABLE
#   - Usa ndimage.affine_transform

# LÃ­nea 202-272: _sample_wms_layer_gpu_approx()
#   - ReproyecciÃ³n WMS â†’ espacio de salida
#   - Loop sobre 4 canales (lÃ­neas 265-270) â—„â”€ OPTIMIZABLE

# LÃ­nea 382-448: render_frame_gpu()
#   - Supersampling fijo 2Ã— (lÃ­nea 405-406)
#   - UI overlay en CPU (lÃ­neas 437-446) â—„â”€ OPTIMIZABLE
```

---

## 2. Cuellos de Botella Identificados

### 2.1 Resumen de Cuellos de Botella

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CUELLOS DE BOTELLA (PRIORIDAD)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  ALTA PRIORIDAD (P0) - Mayor impacto, bajo riesgo                           â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                              â”‚
â”‚  â¶ Loop 4 canales en affine_transform (12 kernels por frame)                â”‚
â”‚     UbicaciÃ³n: _sample_using_inverse_transform() L187-198                   â”‚  
â”‚                _sample_wms_layer_gpu_approx() L265-270                      â”‚
â”‚     Impacto: ~30-40% del tiempo de frame                                    â”‚
â”‚                                                                              â”‚
â”‚  â· AllocaciÃ³n de buffers cada frame                                         â”‚
â”‚     UbicaciÃ³n: cp.zeros() en lÃ­neas 186, 264, 419                           â”‚
â”‚     Impacto: ~5-10% por GC/fragmentaciÃ³n                                    â”‚
â”‚                                                                              â”‚
â”‚  â¸ UI Overlay procesado en CPU cada frame                                   â”‚
â”‚     UbicaciÃ³n: render_frame_gpu() L437-446                                  â”‚
â”‚     Impacto: ~10-15ms por frame (compass + cone + icon)                     â”‚
â”‚                                                                              â”‚
â”‚  MEDIA PRIORIDAD (P1) - Buen impacto, requiere mÃ¡s cuidado                  â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                              â”‚
â”‚  â¹ MÃºltiples alpha_composite separados (3 llamadas)                         â”‚
â”‚     UbicaciÃ³n: render_frame_gpu() L425, L431, L448                          â”‚
â”‚     Impacto: ~10-15ms total                                                 â”‚
â”‚                                                                              â”‚
â”‚  âº ConversiÃ³n Transformer por frame (WMS reprojection)                      â”‚
â”‚     UbicaciÃ³n: _sample_wms_layer_gpu_approx() L238-239                      â”‚
â”‚     Impacto: Overhead de pyproj cada frame                                  â”‚
â”‚                                                                              â”‚
â”‚  BAJA PRIORIDAD (P2) - Mejoras menores                                      â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                              â”‚
â”‚  â» Supersampling fijo 2Ã— (no adaptativo)                                    â”‚
â”‚     UbicaciÃ³n: render_frame_gpu() L405                                      â”‚
â”‚     Impacto: PodrÃ­a reducirse en casos de alta velocidad                    â”‚
â”‚                                                                              â”‚
â”‚  â¼ Dtype float32 para matrices (podrÃ­a ser float16)                         â”‚
â”‚     UbicaciÃ³n: MÃºltiples cp.array(..., dtype=cp.float32)                    â”‚
â”‚     Impacto: ReducciÃ³n de ancho de banda de memoria                         â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 AnÃ¡lisis Detallado de Cuello de Botella â¶

**Problema**: La funciÃ³n `_sample_using_inverse_transform()` ejecuta `ndimage.affine_transform()` **4 veces** (una por canal RGBA):

```python
# CÃ“DIGO ACTUAL (render_gpu.py L187-198)
result_planar = cp.zeros((4, out_h, out_w), dtype=cp.uint8)
for i in range(4):  # â—„â”€ LOOP INEFICIENTE
    ndimage.affine_transform(
        texture_planar[i],
        matrix,
        offset=offset,
        output_shape=(out_h, out_w),
        output=result_planar[i],
        order=1,
        mode='constant',
        cval=0,
        prefilter=False
    )
```

Esto significa **12 llamadas a kernel por frame** (3 capas Ã— 4 canales). Cada llamada tiene:
- Overhead de lanzamiento de kernel (~5-10Î¼s)
- SincronizaciÃ³n implÃ­cita entre kernels
- No se aprovecha el paralelismo entre canales

---

## 3. Optimizaciones de Prioridad Alta (P0)

### 3.1 P0-A: Eliminar Loop de 4 Canales con `map_coordinates`

**Objetivo**: Muestrear los 4 canales RGBA en **una sola operaciÃ³n** GPU.

**SoluciÃ³n**: Usar `cupyx.scipy.ndimage.map_coordinates` con coordenadas pre-calculadas que aplican a todos los canales simultÃ¡neamente.

```python
# PROPUESTA: Reemplazar _sample_using_inverse_transform()

from cupyx.scipy.ndimage import map_coordinates

def _sample_using_inverse_transform_optimized(
    texture_planar: cp.ndarray,  # Shape: (4, H, W)
    center_e: float,
    center_n: float,
    heading: float,
    m_per_px_out: float,
    out_h: int,
    out_w: int,
    ortho_transform: Affine,
    mipmap_level: int = 0
) -> cp.ndarray:
    """
    VersiÃ³n optimizada: muestrea 4 canales en una sola operaciÃ³n GPU.
    
    Cambios respecto al original:
    - Elimina loop for i in range(4)
    - Usa map_coordinates con coordenadas expandidas
    - Mantiene misma salida: (out_h, out_w, 4) en uint8
    """
    # 1. Calcular matriz de transformaciÃ³n (igual que antes)
    vxe, vxn, vye, vyn = _get_transformation_basis(heading, m_per_px_out)
    itf = ~ortho_transform
    level_scale = 1.0 / (2 ** mipmap_level)
    
    cx_tex, cy_tex = itf * (center_e, center_n)
    cx_tex *= level_scale
    cy_tex *= level_scale
    
    d_col_dx = (itf.a * vxe + itf.b * vxn) * level_scale
    d_col_dy = (itf.a * vye + itf.b * vyn) * level_scale
    d_row_dx = (itf.d * vxe + itf.e * vxn) * level_scale
    d_row_dy = (itf.d * vye + itf.e * vyn) * level_scale
    
    # 2. Generar grid de coordenadas de salida
    # Usamos meshgrid en GPU para evitar transferencias
    out_y = cp.arange(out_h, dtype=cp.float32)
    out_x = cp.arange(out_w, dtype=cp.float32)
    grid_y, grid_x = cp.meshgrid(out_y, out_x, indexing='ij')
    
    # Centrar coordenadas
    grid_y_centered = grid_y - out_h / 2.0
    grid_x_centered = grid_x - out_w / 2.0
    
    # 3. Calcular coordenadas de entrada (row, col en textura)
    # Aplicar transformaciÃ³n inversa
    input_row = cy_tex + d_row_dy * grid_y_centered + d_row_dx * grid_x_centered
    input_col = cx_tex + d_col_dy * grid_y_centered + d_col_dx * grid_x_centered
    
    # 4. Stack para map_coordinates: shape (2, out_h, out_w)
    coordinates = cp.stack([input_row, input_col], axis=0)
    
    # 5. Muestrear TODOS los canales en una operaciÃ³n
    # map_coordinates con entrada (4, H, W) produce (4, out_h, out_w)
    result_planar = map_coordinates(
        texture_planar.astype(cp.float32),  # Convertir a float para interpolaciÃ³n
        coordinates,
        order=1,  # Bilinear
        mode='constant',
        cval=0.0,
        prefilter=False
    )
    
    # 6. Convertir de vuelta a uint8 y transponer a (H, W, 4)
    result_planar = cp.clip(result_planar, 0, 255).astype(cp.uint8)
    return cp.transpose(result_planar, (1, 2, 0))
```

**Impacto Esperado**: 
- De 4 llamadas a kernel â†’ 1 llamada
- Ahorro estimado: **25-35% del tiempo de muestreo**

**Riesgo**: âš ï¸ MEDIO - Requiere validaciÃ³n visual para confirmar que el muestreo es idÃ©ntico.

**UbicaciÃ³n del cambio**: `render_gpu.py` lÃ­neas 143-200

---

### 3.2 P0-B: Pre-alocar Buffers de Trabajo en GPURenderContext

**Objetivo**: Eliminar allocaciones repetidas de `cp.zeros()` en cada frame.

**SoluciÃ³n**: AÃ±adir buffers persistentes al `GPURenderContext` que se reutilizan.

```python
# PROPUESTA: Modificar GPURenderContext

class GPURenderContext:
    def __init__(self):
        # ... atributos existentes ...
        
        # NUEVO: Buffers de trabajo pre-alocados
        self._work_buffers_initialized = False
        self._wms_buffer: Optional[cp.ndarray] = None      # (4, max_h, max_w)
        self._ortho_buffer: Optional[cp.ndarray] = None    # (4, max_h, max_w)
        self._vector_buffer: Optional[cp.ndarray] = None   # (4, max_h, max_w)
        self._composite_buffer: Optional[cp.ndarray] = None  # (max_h, max_w, 4)
        self._max_supersampled_size: Tuple[int, int] = (0, 0)
    
    def _ensure_work_buffers(self, ss_width: int, ss_height: int):
        """
        Asegura que los buffers de trabajo estÃ©n alocados para el tamaÃ±o dado.
        Solo re-aloca si el tamaÃ±o actual es insuficiente.
        """
        if (self._work_buffers_initialized and 
            ss_width <= self._max_supersampled_size[0] and
            ss_height <= self._max_supersampled_size[1]):
            return  # Buffers existentes son suficientes
        
        # Alocar con margen del 20% para evitar re-alocaciones frecuentes
        alloc_w = int(ss_width * 1.2)
        alloc_h = int(ss_height * 1.2)
        
        # Free existing if any
        if self._wms_buffer is not None:
            del self._wms_buffer
            del self._ortho_buffer
            del self._vector_buffer
            del self._composite_buffer
            cp.get_default_memory_pool().free_all_blocks()
        
        # Allocate new
        self._wms_buffer = cp.zeros((4, alloc_h, alloc_w), dtype=cp.uint8)
        self._ortho_buffer = cp.zeros((4, alloc_h, alloc_w), dtype=cp.uint8)
        self._vector_buffer = cp.zeros((4, alloc_h, alloc_w), dtype=cp.uint8)
        self._composite_buffer = cp.zeros((alloc_h, alloc_w, 4), dtype=cp.uint8)
        self._max_supersampled_size = (alloc_w, alloc_h)
        self._work_buffers_initialized = True
        
    def get_wms_slice(self, h: int, w: int) -> cp.ndarray:
        """Retorna un slice del buffer WMS del tamaÃ±o requerido."""
        return self._wms_buffer[:, :h, :w]
    
    def get_ortho_slice(self, h: int, w: int) -> cp.ndarray:
        """Retorna un slice del buffer ortho del tamaÃ±o requerido."""
        return self._ortho_buffer[:, :h, :w]
    
    def get_vector_slice(self, h: int, w: int) -> cp.ndarray:
        """Retorna un slice del buffer vector del tamaÃ±o requerido."""
        return self._vector_buffer[:, :h, :w]
    
    def get_composite_slice(self, h: int, w: int) -> cp.ndarray:
        """Retorna un slice del buffer composite del tamaÃ±o requerido."""
        return self._composite_buffer[:h, :w, :]
```

**Uso en render_frame_gpu()**:

```python
def render_frame_gpu(...):
    # ...
    ss_factor = 2
    sw, sh = width * ss_factor, height * ss_factor
    
    # NUEVO: Asegurar buffers y usarlos
    _CONTEXT._ensure_work_buffers(sw, sh)
    
    # En lugar de: final_gpu = cp.zeros((sh, sw, 4), dtype=cp.uint8)
    # Usar slice del buffer pre-alocado:
    final_gpu = _CONTEXT.get_composite_slice(sh, sw)
    final_gpu.fill(0)  # Limpiar en lugar de alocar
    # ...
```

**Impacto Esperado**:
- Elimina overhead de allocaciÃ³n en cada frame
- Reduce fragmentaciÃ³n de memoria GPU
- Ahorro estimado: **5-10% del tiempo total**

**Riesgo**: âš¡ BAJO - Cambio de memoria, no de lÃ³gica de renderizado.

**UbicaciÃ³n del cambio**: `render_gpu.py` lÃ­neas 277-302 (GPURenderContext)

---

### 3.3 P0-C: Pre-renderizar Compass a CachÃ© GPU (360Â°)

**Objetivo**: Eliminar el dibujado del compass en CPU en cada frame.

**Problema Actual** (lÃ­neas 437-446):
```python
# CÃ“DIGO ACTUAL - CPU cada frame
ui_layer = Image.new("RGBA", (width, height), (0, 0, 0, 0))
# ...
if show_compass:
    c_sz = max(15, compass_size_px)
    c_pos = (width - (c_sz // 2 + 15), c_sz // 2 + 15)
    _draw_compass(ui_layer, c_pos, c_sz, -heading)  # â—„â”€ CPU CADA FRAME

ui_gpu = cp.asarray(np.array(ui_layer))  # â—„â”€ Transferencia CPUâ†’GPU
```

**SoluciÃ³n**: Pre-renderizar 360 imÃ¡genes del compass (1 por grado) durante `preload()`.

```python
# PROPUESTA: Agregar a GPURenderContext

class GPURenderContext:
    def __init__(self):
        # ... existentes ...
        
        # NUEVO: CachÃ© de compass pre-renderizado
        self._compass_cache: Optional[cp.ndarray] = None  # Shape: (360, size, size, 4)
        self._compass_size: int = 0
    
    def _pre_render_compass_cache(self, compass_size: int):
        """
        Pre-renderiza el compass para los 360 grados.
        Se ejecuta una sola vez durante preload().
        """
        if self._compass_cache is not None and self._compass_size == compass_size:
            return  # Ya estÃ¡ cacheado
        
        from render import _draw_compass
        from PIL import Image
        import numpy as np
        
        # Crear array en CPU primero (360 imÃ¡genes)
        cache_cpu = np.zeros((360, compass_size, compass_size, 4), dtype=np.uint8)
        
        for heading in range(360):
            img = Image.new("RGBA", (compass_size, compass_size), (0, 0, 0, 0))
            center = (compass_size // 2, compass_size // 2)
            _draw_compass(img, center, compass_size, -heading)
            cache_cpu[heading] = np.array(img)
        
        # Subir TODO el cachÃ© a GPU de una vez
        self._compass_cache = cp.asarray(cache_cpu)
        self._compass_size = compass_size
        
        print(f"[GPU] Compass cache: {360} rotaciones pre-renderizadas ({compass_size}x{compass_size})")
    
    def get_compass_for_heading(self, heading: float) -> cp.ndarray:
        """
        Obtiene el compass pre-renderizado para el heading dado.
        Retorna: (size, size, 4) uint8 array en GPU
        """
        heading_int = int(heading) % 360
        return self._compass_cache[heading_int]
```

**Uso en preload()**:

```python
def preload(self, dataset, center_points, margin_m, vectors=None, 
            arrow_size=100, cone_len=200, wms_source="google_hybrid", 
            icon_opacity=0.4, progress_callback=None,
            compass_size=40):  # â—„â”€ Nuevo parÃ¡metro
    # ... cÃ³digo existente ...
    
    notify(95, "Pre-renderizando compass...")  # NUEVO
    self._pre_render_compass_cache(compass_size)
    
    notify(100, "Precarga GPU completada.")
    self.is_ready = True
```

**Uso en render_frame_gpu()**:

```python
def render_frame_gpu(...):
    # ... cÃ³digo existente hasta UI overlay ...
    
    if show_compass and _CONTEXT._compass_cache is not None:
        # NUEVO: Obtener compass pre-renderizado de GPU
        c_sz = max(15, compass_size_px)
        compass_gpu = _CONTEXT.get_compass_for_heading(heading)
        
        # Posicionar en la esquina superior derecha
        x_offset = width - c_sz - 15
        y_offset = 15
        
        # Composite directo en GPU (sin transferencia CPU)
        compass_region = final_gpu[y_offset:y_offset+c_sz, x_offset:x_offset+c_sz, :]
        final_gpu[y_offset:y_offset+c_sz, x_offset:x_offset+c_sz, :] = \
            _alpha_composite_gpu(compass_gpu, compass_region)
    
    # ... resto del cÃ³digo ...
```

**Impacto Esperado**:
- Elimina `_draw_compass()` por frame (~5-10ms)
- Elimina conversiÃ³n PILâ†’numpyâ†’cupy (~2-3ms)
- Ahorro total estimado: **10-15ms por frame**

**Riesgo**: âš¡ BAJO - Solo cambia cÃ³mo se obtiene el compass, no el resultado visual.

**Memoria GPU Adicional**: ~360 Ã— 40 Ã— 40 Ã— 4 = 2.3 MB (insignificante)

---

## 4. Optimizaciones de Prioridad Media (P1)

### 4.1 P1-A: Fusionar Alpha Compositing en OperaciÃ³n Ãšnica

**Problema Actual**: 3 llamadas separadas a `_alpha_composite_gpu()`:

```python
# CÃ“DIGO ACTUAL (render_frame_gpu)
final_gpu = _alpha_composite_gpu(ortho_layer, final_gpu)   # 1
final_gpu = _alpha_composite_gpu(vec_layer, final_gpu)     # 2
# ... despuÃ©s ...
return _alpha_composite_gpu(ui_gpu, final_gpu)              # 3
```

**SoluciÃ³n**: Crear funciÃ³n de composiciÃ³n multi-capa.

```python
# PROPUESTA: Nueva funciÃ³n de composiciÃ³n fusionada

def _alpha_composite_multi_gpu(
    layers: List[cp.ndarray],
    order: str = "back_to_front"
) -> cp.ndarray:
    """
    Compone mÃºltiples capas RGBA en una sola operaciÃ³n optimizada.
    
    Args:
        layers: Lista de arrays (H, W, 4) en orden de composiciÃ³n
        order: "back_to_front" (primero es fondo) o "front_to_back"
    
    Returns:
        Array resultante (H, W, 4) en uint8
    
    OptimizaciÃ³n: Evita crear arrays intermedios innecesarios.
    """
    if len(layers) == 0:
        raise ValueError("Se requiere al menos una capa")
    if len(layers) == 1:
        return layers[0].copy()
    
    # Convertir todas a float32 de una vez
    alphas = [layer[:, :, 3:4].astype(cp.float32) / 255.0 for layer in layers]
    rgbs = [layer[:, :, :3].astype(cp.float32) for layer in layers]
    
    # Empezar desde el fondo
    if order == "back_to_front":
        result_rgb = rgbs[0].copy()
        result_a = alphas[0].copy()
        
        for i in range(1, len(layers)):
            fg_a = alphas[i]
            fg_rgb = rgbs[i]
            
            # Porter-Duff "over"
            inv_fg_a = 1.0 - fg_a
            out_a = fg_a + result_a * inv_fg_a
            out_a_safe = cp.where(out_a > 1e-6, out_a, 1.0)
            
            result_rgb = (fg_rgb * fg_a + result_rgb * result_a * inv_fg_a) / out_a_safe
            result_a = out_a
    else:
        raise NotImplementedError("front_to_back no implementado")
    
    # Convertir de vuelta a uint8
    return cp.dstack((
        cp.clip(result_rgb, 0, 255).astype(cp.uint8),
        cp.clip(result_a * 255, 0, 255).astype(cp.uint8)
    ))
```

**Uso en render_frame_gpu()**:

```python
# ANTES:
# final_gpu = _alpha_composite_gpu(ortho_layer, final_gpu)
# final_gpu = _alpha_composite_gpu(vec_layer, final_gpu) 
# return _alpha_composite_gpu(ui_gpu, final_gpu)

# DESPUÃ‰S:
layers = [wms_layer, ortho_layer, vec_layer, ui_layer_gpu]
return _alpha_composite_multi_gpu(layers, order="back_to_front")
```

**Impacto Esperado**:
- Reduce conversiones float32â†”uint8 intermedias
- Menos arrays temporales = menos presiÃ³n de memoria
- Ahorro estimado: **5-8ms por frame**

**Riesgo**: âš ï¸ MEDIO - Requiere validar que el orden de composiciÃ³n es correcto.

---

### 4.2 P1-B: Cachear Transformer de ProyecciÃ³n WMS

**Problema Actual**: Se crea un `Transformer` nuevo en cada frame:

```python
# CÃ“DIGO ACTUAL (_sample_wms_layer_gpu_approx L238-239)
from_crs = CRS.from_user_input(ortho_crs)
transformer = Transformer.from_crs(from_crs, "EPSG:4326", always_xy=True)
```

**SoluciÃ³n**: Cachear el transformer en `GPURenderContext`.

```python
# PROPUESTA: Modificar GPURenderContext

class GPURenderContext:
    def __init__(self):
        # ... existentes ...
        
        # NUEVO: Transformer cacheado
        self._wms_transformer: Optional[Transformer] = None
        self._wms_from_crs_str: Optional[str] = None
    
    def get_wms_transformer(self, ortho_crs) -> Transformer:
        """
        Obtiene el transformer para WMS, cacheÃ¡ndolo si es posible.
        """
        from pyproj import CRS, Transformer
        
        crs_str = str(ortho_crs)
        if self._wms_transformer is not None and self._wms_from_crs_str == crs_str:
            return self._wms_transformer
        
        from_crs = CRS.from_user_input(ortho_crs)
        self._wms_transformer = Transformer.from_crs(from_crs, "EPSG:4326", always_xy=True)
        self._wms_from_crs_str = crs_str
        return self._wms_transformer
```

**Uso en _sample_wms_layer_gpu_approx()**:

```python
def _sample_wms_layer_gpu_approx(..., context: GPURenderContext = None):
    # ...
    
    # ANTES:
    # from_crs = CRS.from_user_input(ortho_crs)
    # transformer = Transformer.from_crs(from_crs, "EPSG:4326", always_xy=True)
    
    # DESPUÃ‰S:
    transformer = _CONTEXT.get_wms_transformer(ortho_crs)
    
    # ... resto igual ...
```

**Impacto Esperado**:
- Elimina creaciÃ³n de objetos pyproj cada frame
- Ahorro estimado: **2-3ms por frame**

**Riesgo**: âš¡ BAJO - Solo cachÃ© de objeto inmutable.

---

### 4.3 P1-C: Pre-calcular Matriz de TransformaciÃ³n AfÃ­n

**OptimizaciÃ³n menor**: Los cÃ¡lculos de `d_col_dx`, `d_row_dy`, etc. son determinÃ­sticos dado `(heading, m_per_px_out, ortho_transform)`. Se pueden cachear para headings similares.

```python
# PROPUESTA: Agregar cachÃ© de matrices afines

class GPURenderContext:
    def __init__(self):
        # ... existentes ...
        self._affine_matrix_cache: Dict[Tuple[int, int, int], Tuple[cp.ndarray, cp.ndarray]] = {}
        self._affine_cache_max_size: int = 180  # Limitar a 180 entradas
    
    def get_cached_affine(
        self, 
        heading: float, 
        m_per_px_out: float, 
        mipmap_level: int
    ) -> Optional[Tuple[cp.ndarray, cp.ndarray]]:
        """
        Obtiene matriz y offset cacheados si existen.
        Discretiza heading a 2 decimales para aumentar cache hits.
        """
        # Discretizar para aumentar hits
        key = (
            round(heading, 1),  # Redondear heading a 0.1 grados
            round(m_per_px_out * 1000),  # Discretizar a mm
            mipmap_level
        )
        return self._affine_matrix_cache.get(key)
    
    def cache_affine(
        self,
        heading: float,
        m_per_px_out: float,
        mipmap_level: int,
        matrix: cp.ndarray,
        offset: cp.ndarray
    ):
        """Cachea matriz y offset para uso futuro."""
        if len(self._affine_matrix_cache) >= self._affine_cache_max_size:
            # LRU simple: eliminar primera entrada
            first_key = next(iter(self._affine_matrix_cache))
            del self._affine_matrix_cache[first_key]
        
        key = (
            round(heading, 1),
            round(m_per_px_out * 1000),
            mipmap_level
        )
        self._affine_matrix_cache[key] = (matrix.copy(), offset.copy())
```

**Impacto Esperado**: 
- Menor para tracks con headings variados
- Significativo para tracks con headings repetidos
- Ahorro estimado: **1-3ms por frame** en mejores casos

---

## 5. Optimizaciones de Prioridad Baja (P2)

### 5.1 P2-A: Supersampling Adaptativo

**Idea**: Reducir supersampling cuando el movimiento es rÃ¡pido (motion blur oculta aliasing).

```python
# PROPUESTA: Modificar render_frame_gpu()

def _compute_adaptive_supersample(
    velocity: Optional[float] = None,
    frame_rate: int = 30
) -> int:
    """
    Calcula factor de supersampling Ã³ptimo.
    
    Args:
        velocity: Velocidad en m/s (None = usar default)
        frame_rate: FPS del video
    
    Returns:
        Factor de supersampling (1 o 2)
    """
    if velocity is None:
        return 2  # Default actual
    
    # A velocidades altas, el motion blur natural oculta el aliasing
    # Umbral empÃ­rico: > 15 m/s = ~54 km/h
    if velocity > 15:
        return 1
    return 2
```

**NOTA**: Esta optimizaciÃ³n requiere que el sistema de jobs pase la velocidad al renderizador, lo cual es un cambio de interfaz. Se recomienda como optimizaciÃ³n futura.

**Impacto Potencial**: Reducir resoluciÃ³n interna 4Ã— (de 960Ã—960 a 480Ã—480) = **~30-40% mÃ¡s rÃ¡pido** en frames de alta velocidad.

**Riesgo**: âš ï¸ MEDIO - Puede introducir aliasing visible. Requiere pruebas extensivas.

---

### 5.2 P2-B: Usar Float16 para Muestreo

**Idea**: Reducir precision de float32 a float16 para operaciones de muestreo.

```python
# PROPUESTA: Modificar _sample_using_inverse_transform()

# En lugar de:
matrix = cp.array([...], dtype=cp.float32)

# Usar:
matrix = cp.array([...], dtype=cp.float16)  # Half precision

# NOTA: map_coordinates de CuPy acepta float16
```

**Impacto Potencial**:
- 2Ã— menos ancho de banda de memoria
- RTX 3050 tiene buen soporte FP16
- Ahorro estimado: **5-10%** en operaciones memory-bound

**Riesgo**: âš¡ BAJO para muestreo de texturas (la precisiÃ³n no afecta visualmente).

**NOTA**: Requiere pruebas para confirmar que no hay artefactos en coordenadas extremas.

---

## 6. Orden de ImplementaciÃ³n Recomendado

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PLAN DE IMPLEMENTACIÃ“N SUGERIDO                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  FASE 1: Quick Wins (1-2 dÃ­as)                                               â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                                â”‚
â”‚  â–¡ P0-B: Pre-alocar buffers de trabajo                                       â”‚
â”‚    â†’ Bajo riesgo, fÃ¡cil implementaciÃ³n                                       â”‚
â”‚    â†’ Impacto: ~5-10% mejora                                                  â”‚
â”‚                                                                              â”‚
â”‚  â–¡ P1-B: Cachear Transformer de WMS                                          â”‚
â”‚    â†’ Cambio trivial, 0 riesgo                                                â”‚
â”‚    â†’ Impacto: ~2-3ms por frame                                               â”‚
â”‚                                                                              â”‚
â”‚  â–¡ P0-C: Pre-renderizar compass 360Â°                                         â”‚
â”‚    â†’ Bajo riesgo, alto impacto                                               â”‚
â”‚    â†’ Impacto: ~10-15ms por frame                                             â”‚
â”‚                                                                              â”‚
â”‚  FASE 2: OptimizaciÃ³n Core (2-3 dÃ­as)                                        â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                           â”‚
â”‚  â–¡ P0-A: Eliminar loop 4 canales con map_coordinates                         â”‚
â”‚    â†’ Requiere testing visual cuidadoso                                       â”‚
â”‚    â†’ ALTO impacto: ~25-35% mejora en muestreo                                â”‚
â”‚                                                                              â”‚
â”‚  â–¡ P1-A: Fusionar alpha compositing                                          â”‚
â”‚    â†’ Requiere validar orden de capas                                         â”‚
â”‚    â†’ Impacto: ~5-8ms por frame                                               â”‚
â”‚                                                                              â”‚
â”‚  FASE 3: Polish (opcional)                                                   â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                                   â”‚
â”‚  â–¡ P1-C: Cachear matrices afines                                             â”‚
â”‚  â–¡ P2-B: Float16 para muestreo                                               â”‚
â”‚  â–¡ P2-A: Supersampling adaptativo (requiere cambios de interfaz)             â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### EstimaciÃ³n de Mejora Acumulativa

| Fase | Optimizaciones | FPS Estimados* | Mejora |
|------|---------------|----------------|--------|
| Actual | Baseline | ~5-6 FPS | - |
| Fase 1 | P0-B, P1-B, P0-C | ~8-10 FPS | +40-60% |
| Fase 2 | + P0-A, P1-A | ~12-15 FPS | +50-80% |
| Fase 3 | + P1-C, P2-B | ~15-18 FPS | +15-20% |

*Estimaciones basadas en anÃ¡lisis de cÃ³digo. Resultados reales dependen del hardware y contenido.

---

## 7. Notas de Seguridad

### 7.1 QuÃ© NO Modificar

| Archivo | RazÃ³n |
|---------|-------|
| `render.py` | LÃ³gica CPU de fallback - INTOCABLE |
| `app.py` (endpoints) | API pÃºblica - NO cambiar firmas |
| `models.py` | Modelos de datos - NO modificar |
| `web/` (frontend) | Funciona correctamente - NO tocar |

### 7.2 ValidaciÃ³n OBLIGATORIA: 3 Tests DespuÃ©s de Cada Cambio

> âš ï¸ **CRÃTICO**: DespuÃ©s de implementar CUALQUIER optimizaciÃ³n de este documento, se DEBEN ejecutar los siguientes 3 tests de validaciÃ³n usando los datos de prueba ubicados en:
> 
> **`D:\Dev\MinimapaGPU\backend\gpu_validation\`**

#### Archivos de ValidaciÃ³n Disponibles

| Archivo | DescripciÃ³n | Uso |
|---------|-------------|-----|
| `test_ortho_crop.tif` | Ortomosaico de prueba (15.5 MB) | Input principal para tests |
| `LinderoGeneral.geojson` | Vectores de linderos (137 KB) | Test de renderizado de vectores |
| `Vias.geojson` | Vectores de vÃ­as (200 KB) | Test de renderizado de lÃ­neas |
| `render_cpu.png` | Referencia de renderizado CPU | ComparaciÃ³n visual baseline |
| `render_gpu_fixed.png` | Referencia GPU correcta | ComparaciÃ³n para regresiones |
| `diff.png` | Imagen de diferencias | Referencia de tolerancia aceptable |
| `pipe_test.mp4` | Video de referencia funcional | ValidaciÃ³n de pipeline completo |

---

#### TEST 1: ValidaciÃ³n de Frame EstÃ¡tico (ComparaciÃ³n Visual)

**Objetivo**: Verificar que el frame renderizado GPU sea visualmente idÃ©ntico al CPU.

```python
# TEST 1: Ejecutar desde backend/
# Comando sugerido (adaptar segÃºn implementaciÃ³n actual)

import numpy as np
from PIL import Image
from pathlib import Path

def test_static_frame_comparison():
    """
    Compara un frame GPU vs la referencia CPU.
    DEBE ejecutarse despuÃ©s de cada cambio.
    """
    VALIDATION_DIR = Path("gpu_validation")
    
    # 1. Renderizar frame con configuraciÃ³n estÃ¡ndar
    # (usar test_ortho_crop.tif, LinderoGeneral.geojson, Vias.geojson)
    
    # 2. Cargar referencia CPU
    ref_cpu = np.array(Image.open(VALIDATION_DIR / "render_cpu.png"))
    
    # 3. Comparar pixel a pixel
    # Tolerancia: Â±3 por canal (para diferencias de interpolaciÃ³n)
    diff = np.abs(rendered_gpu.astype(np.int16) - ref_cpu.astype(np.int16))
    max_diff = diff.max()
    mean_diff = diff.mean()
    
    # 4. Criterios de PASS/FAIL
    assert max_diff <= 10, f"FAIL: Diferencia mÃ¡xima {max_diff} > 10"
    assert mean_diff <= 1.0, f"FAIL: Diferencia media {mean_diff} > 1.0"
    
    print(f"âœ… TEST 1 PASSED: max_diff={max_diff}, mean_diff={mean_diff:.3f}")
```

**Criterio de AceptaciÃ³n**: 
- Diferencia mÃ¡xima por pixel â‰¤ 10 (de 255)
- Diferencia media â‰¤ 1.0

---

#### TEST 2: ValidaciÃ³n de Video Completo (Pipeline E2E)

**Objetivo**: Generar un video corto y verificar integridad del pipeline.

```python
def test_video_pipeline():
    """
    Genera un video de 5 segundos (150 frames @ 30fps) y valida:
    - No hay frames corruptos
    - Video es reproducible
    - TamaÃ±o de archivo razonable
    """
    VALIDATION_DIR = Path("gpu_validation")
    
    # 1. Generar video usando:
    #    - Ortho: test_ortho_crop.tif
    #    - Vectores: LinderoGeneral.geojson + Vias.geojson
    #    - Track: ruta circular simple (generar programÃ¡ticamente)
    #    - WMS: google_hybrid
    
    # 2. Validar que el archivo existe y tiene tamaÃ±o > 100KB
    output = Path("gpu_validation/test_output.mp4")
    assert output.exists(), "FAIL: Video no generado"
    assert output.stat().st_size > 100_000, "FAIL: Video muy pequeÃ±o (corrupto?)"
    
    # 3. Validar con ffprobe que es reproducible
    import subprocess
    result = subprocess.run(
        ["ffprobe", "-v", "error", "-show_entries", "format=duration", str(output)],
        capture_output=True, text=True
    )
    assert "duration=" in result.stdout, "FAIL: Video no es reproducible"
    
    print(f"âœ… TEST 2 PASSED: Video generado correctamente")
```

**Criterio de AceptaciÃ³n**:
- Video generado sin errores
- Archivo > 100 KB
- FFprobe puede leer duraciÃ³n

---

#### TEST 3: ValidaciÃ³n de Performance (Benchmark)

**Objetivo**: Verificar que la optimizaciÃ³n NO degrada el rendimiento.

```python
import time

def test_performance_benchmark():
    """
    Mide tiempo de renderizado de 100 frames.
    Compara contra baseline para detectar regresiones.
    """
    BASELINE_FPS = 5.0  # FPS mÃ­nimo aceptable (ajustar segÃºn baseline actual)
    NUM_FRAMES = 100
    
    # 1. Precarga
    # preload_track_gpu(...)
    
    # 2. Renderizar 100 frames y medir tiempo
    start = time.perf_counter()
    for i in range(NUM_FRAMES):
        # render_frame_gpu(...)
        pass
    elapsed = time.perf_counter() - start
    
    # 3. Calcular FPS
    fps = NUM_FRAMES / elapsed
    
    # 4. Validar que no hay regresiÃ³n
    assert fps >= BASELINE_FPS * 0.9, f"FAIL: RegresiÃ³n de performance! {fps:.1f} < {BASELINE_FPS * 0.9:.1f}"
    
    print(f"âœ… TEST 3 PASSED: {fps:.1f} FPS (baseline: {BASELINE_FPS} FPS)")
    
    # 5. Reportar mejora si la hay
    if fps > BASELINE_FPS * 1.1:
        improvement = ((fps / BASELINE_FPS) - 1) * 100
        print(f"ğŸš€ MEJORA DETECTADA: +{improvement:.1f}%")
```

**Criterio de AceptaciÃ³n**:
- FPS â‰¥ 90% del baseline
- Idealmente: FPS > baseline

---

### 7.3 Criterios de AceptaciÃ³n del Video Final

> âš ï¸ **OBLIGATORIO**: El video generado DEBE contener TODOS los siguientes elementos visuales, tal como funciona la implementaciÃ³n actual.

#### Elementos OBLIGATORIOS en el Video

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CHECKLIST DE ELEMENTOS VISUALES OBLIGATORIOS                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â˜‘ BRÃšJULA (Compass)                                                         â”‚
â”‚    â””â”€ Debe aparecer en esquina superior derecha                              â”‚
â”‚    â””â”€ Debe rotar correctamente segÃºn heading                                 â”‚
â”‚    â””â”€ Debe tener el diseÃ±o actual (N, S, E, W visibles)                     â”‚
â”‚                                                                              â”‚
â”‚  â˜‘ ICONO DE NAVEGACIÃ“N (Nav Icon)                                           â”‚
â”‚    â””â”€ Debe aparecer centrado en el frame                                     â”‚
â”‚    â””â”€ Flecha/triÃ¡ngulo apuntando hacia arriba (direcciÃ³n de viaje)          â”‚
â”‚                                                                              â”‚
â”‚  â˜‘ CÃRCULO OPACO DEL ICONO                                                  â”‚
â”‚    â””â”€ CÃ­rculo semitransparente detrÃ¡s del icono                              â”‚
â”‚    â””â”€ Opacidad configurable (icon_circle_opacity)                            â”‚
â”‚    â””â”€ TamaÃ±o configurable (icon_circle_size_px)                              â”‚
â”‚                                                                              â”‚
â”‚  â˜‘ CONO DE VISIÃ“N (Vision Cone)                                             â”‚
â”‚    â””â”€ Cono semitransparente apuntando hacia adelante                         â”‚
â”‚    â””â”€ Ãngulo y longitud configurables                                        â”‚
â”‚                                                                              â”‚
â”‚  â˜‘ VECTORES GEOJSON                                                         â”‚
â”‚    â””â”€ LÃ­neas de LinderoGeneral.geojson visibles                              â”‚
â”‚    â””â”€ LÃ­neas de Vias.geojson visibles                                        â”‚
â”‚    â””â”€ Colores correctos segÃºn configuraciÃ³n                                  â”‚
â”‚    â””â”€ Grosor de lÃ­nea correcto                                               â”‚
â”‚                                                                              â”‚
â”‚  â˜‘ MAPA BASE SATELITAL (WMS Layer)                                          â”‚
â”‚    â””â”€ Imagen satelital visible como fondo                                    â”‚
â”‚    â””â”€ Fuente seleccionada (google_hybrid, esri, bing, etc.)                 â”‚
â”‚    â””â”€ ResoluciÃ³n apropiada para el nivel de zoom                             â”‚
â”‚                                                                              â”‚
â”‚  â˜‘ ORTOMOSAICO TIFF                                                         â”‚
â”‚    â””â”€ Imagen del TIFF cargado superpuesta sobre WMS                          â”‚
â”‚    â””â”€ Transparencia correcta en bordes                                       â”‚
â”‚    â””â”€ Colores fieles al original                                             â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Defectos NO PERMITIDOS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEFECTOS QUE CAUSAN RECHAZO                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  âœ– HUECOS NEGROS (Black Holes)                                              â”‚
â”‚    â””â”€ NO debe haber Ã¡reas negras donde deberÃ­a verse contenido               â”‚
â”‚    â””â”€ Indica: fallo en muestreo o coordenadas fuera de rango                â”‚
â”‚                                                                              â”‚
â”‚  âœ– JITTER / TEMBLOR (Temporal Instability)                                  â”‚
â”‚    â””â”€ NO debe haber temblor o vibraciÃ³n entre frames consecutivos            â”‚
â”‚    â””â”€ Indica: inconsistencia en cÃ¡lculos de transformaciÃ³n                  â”‚
â”‚    â””â”€ El movimiento debe ser suave y continuo                                â”‚
â”‚                                                                              â”‚
â”‚  âœ– COSTURAS VISIBLES (Seams/Stitching Artifacts)                            â”‚
â”‚    â””â”€ NO debe haber lÃ­neas o bordes visibles entre capas                     â”‚
â”‚    â””â”€ Ortho y WMS deben fusionarse sin costuras                              â”‚
â”‚    â””â”€ Indica: error en alineaciÃ³n de coordenadas                            â”‚
â”‚                                                                              â”‚
â”‚  âœ– ZOOM INCORRECTO ENTRE CAPAS                                              â”‚
â”‚    â””â”€ Ortho y WMS deben tener el MISMO nivel de zoom visual                  â”‚
â”‚    â””â”€ NO debe verse una capa "mÃ¡s cerca" que otra                            â”‚
â”‚    â””â”€ Indica: error en cÃ¡lculo de m_per_px o escala                         â”‚
â”‚                                                                              â”‚
â”‚  âœ– DESALINEACIÃ“N ESPACIAL                                                   â”‚
â”‚    â””â”€ Las capas deben estar perfectamente alineadas geogrÃ¡ficamente          â”‚
â”‚    â””â”€ Los vectores deben coincidir con los features del TIFF                 â”‚
â”‚    â””â”€ Indica: error en transformaciÃ³n de coordenadas                        â”‚
â”‚                                                                              â”‚
â”‚  âœ– ELEMENTOS UI FALTANTES                                                   â”‚
â”‚    â””â”€ BrÃºjula, icono, cono, cÃ­rculo DEBEN estar presentes                    â”‚
â”‚    â””â”€ Indica: regresiÃ³n en cÃ³digo de UI overlay                              â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ComparaciÃ³n Visual de Referencia

Para validar visualmente, comparar contra:

| Referencia | Ruta | DescripciÃ³n |
|------------|------|-------------|
| **CPU Baseline** | `gpu_validation/render_cpu.png` | Frame correcto renderizado por CPU |
| **GPU Correcto** | `gpu_validation/render_gpu_fixed.png` | Frame GPU que pasa validaciÃ³n |
| **Diff Aceptable** | `gpu_validation/diff.png` | Diferencias tolerables entre CPU/GPU |
| **Video Funcional** | `gpu_validation/pipe_test.mp4` | Video completo que funciona correctamente |

---

### 7.4 Rollback Plan

- **Git**: Cada optimizaciÃ³n debe ser un commit separado
- **Feature flags**: Considerar `USE_OPTIMIZED_SAMPLING = True/False`
- **Logging**: Agregar logs de performance para comparar antes/despuÃ©s
- **Backup**: Antes de cada cambio, asegurar que el commit anterior genera video correcto

---

### 7.5 Flujo de Trabajo Obligatorio

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                FLUJO DE TRABAJO PARA CADA OPTIMIZACIÃ“N                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  1. ANTES de modificar cÃ³digo:                                               â”‚
â”‚     â–¡ Ejecutar TEST 1, 2, 3 con cÃ³digo ACTUAL                                â”‚
â”‚     â–¡ Guardar resultados como BASELINE                                       â”‚
â”‚     â–¡ Hacer commit del estado actual (punto de rollback)                     â”‚
â”‚                                                                              â”‚
â”‚  2. IMPLEMENTAR la optimizaciÃ³n:                                             â”‚
â”‚     â–¡ Seguir propuesta del documento                                         â”‚
â”‚     â–¡ Documentar cualquier desviaciÃ³n                                        â”‚
â”‚                                                                              â”‚
â”‚  3. DESPUÃ‰S de modificar cÃ³digo:                                             â”‚
â”‚     â–¡ Ejecutar TEST 1: ComparaciÃ³n visual â†’ DEBE PASAR                       â”‚
â”‚     â–¡ Ejecutar TEST 2: Pipeline E2E â†’ DEBE PASAR                             â”‚
â”‚     â–¡ Ejecutar TEST 3: Performance â†’ DEBE PASAR (sin regresiÃ³n)              â”‚
â”‚                                                                              â”‚
â”‚  4. VALIDACIÃ“N VISUAL del video:                                             â”‚
â”‚     â–¡ Reproducir video generado                                              â”‚
â”‚     â–¡ Verificar checklist de elementos obligatorios                          â”‚
â”‚     â–¡ Verificar ausencia de defectos prohibidos                              â”‚
â”‚                                                                              â”‚
â”‚  5. Si TODO pasa:                                                            â”‚
â”‚     â–¡ Hacer commit con mensaje descriptivo                                   â”‚
â”‚     â–¡ Actualizar baseline de performance si mejorÃ³                           â”‚
â”‚                                                                              â”‚
â”‚  6. Si ALGO falla:                                                           â”‚
â”‚     â–¡ git checkout al commit anterior                                        â”‚
â”‚     â–¡ Analizar causa del fallo                                               â”‚
â”‚     â–¡ Ajustar implementaciÃ³n y repetir desde paso 2                          â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Resumen Final

### Optimizaciones Recomendadas para ImplementaciÃ³n Inmediata

1. **P0-B (Buffers pre-alocados)**: FÃ¡cil, bajo riesgo, mejora de memoria
2. **P0-C (Compass cache)**: Alto impacto, bajo riesgo
3. **P1-B (Transformer cache)**: Trivial, 0 riesgo

### Optimizaciones que Requieren Testing Cuidadoso

1. **P0-A (map_coordinates unificado)**: Mayor impacto pero requiere validaciÃ³n visual
2. **P1-A (Compositing fusionado)**: Requiere validar orden de capas

### Optimizaciones Opcionales/Futuras

1. **P2-A (Supersampling adaptativo)**: Requiere cambios de interfaz
2. **P2-B (Float16)**: Beneficio menor, requiere validaciÃ³n de precisiÃ³n

---

*Documento generado para implementaciÃ³n por agente AI.*
*Ãšltima actualizaciÃ³n: 2026-01-29*
*Basado en anÃ¡lisis de: render_gpu.py, app.py, gpu_utils.py*
